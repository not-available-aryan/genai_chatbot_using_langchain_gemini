{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational UI Chatbot App with Gemini, LangChain and Streamlit\n",
        "\n",
        "Here we will build a advanced Conversational UI-based chatbot using LangChain and Streamlit with the following features:\n",
        "\n",
        "- Custom Landing Page\n",
        "- Conversational memory"
      ],
      "metadata": {
        "id": "xPRVq3e03c1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install App and LLM dependencies"
      ],
      "metadata": {
        "id": "L1KvMtf54l0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.1.12 -q\n",
        "!pip install langchain-google-genai==0.0.7 -q\n",
        "!pip install langchain-community==0.0.29 -q\n",
        "!pip install streamlit==1.32.2 -q\n",
        "!pip install pyngrok==7.1.5 -q\n",
        "!pip install google-generativeai>=0.3.2 -q"
      ],
      "metadata": {
        "id": "2evPp14fy258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e429f6df-c15c-468a-ec57-c2ade6837147"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/dataclasses-json/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/langchain-community/\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/langchain-core/\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m908.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.1/809.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m824.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.9/146.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m598.7/598.7 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.6 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Gemini API Credentials\n",
        "\n",
        "Here we load it from a file so we don't explore the credentials on the internet by mistake"
      ],
      "metadata": {
        "id": "CiwGjVWK4q6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "kDe44J0N0NcC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Streamlit UI without Memory\n",
        "\n",
        "This simple version shows how to:\n",
        "\n",
        "1. Create a basic Streamlit interface\n",
        "2. Connect directly to Gemini\n",
        "3. Process basic Q&A without memory"
      ],
      "metadata": {
        "id": "-Eio-LoBKnzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile basic_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"Basic AI Assistant (No Memory)\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Simple input/output\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        response = gemini.invoke(user_input)\n",
        "        st.write(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBfzThOWKu42",
        "outputId": "0a09d155-30ee-4346-c9ac-55f1748f175c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing basic_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "6naokMhCMiPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run basic_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "UZGYWmcfMiPF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d5e938e-2602-4930-97b0-d066bfcb0f14",
        "id": "WR65GrUEMrcD"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://378c-34-138-160-11.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "I_k4wNzQMrcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "NJhCkMhyMrcH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8125931-3757-4b52-c0d7-4cbb92dfa981",
        "id": "LT8xZ404MrcH"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2463       1  1 09:19 ?        00:00:04 /usr/bin/python3 /usr/local/bin/streamlit run basic_app.py --server.port=8989\n",
            "root        3971    1533  0 09:25 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        3973    3971  0 09:25 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 2463"
      ],
      "metadata": {
        "id": "kIjWRCybMrcI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Showing Messages with Manual Memory\n",
        "\n",
        "This demonstrates:\n",
        "\n",
        "1. How to manually implement memory in Streamlit\n",
        "2. The challenge of formatting context for the LLM\n",
        "3. Why specialized tools like LangChain help"
      ],
      "metadata": {
        "id": "-svMcbHaNLpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile manual_app.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "st.title(\"AI Assistant with Manual Memory\")\n",
        "\n",
        "# Initialize the LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1)\n",
        "\n",
        "# Initialize session state for memory\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display existing messages\n",
        "for message in st.session_state.messages:\n",
        "    st.chat_message(message[\"role\"]).write(message[\"content\"])\n",
        "\n",
        "# Process new input\n",
        "if user_input := st.chat_input(\"Ask a question\"):\n",
        "    # Add user message to history\n",
        "    st.session_state.messages.append({\"role\": \"human\", \"content\": user_input})\n",
        "    st.chat_message(\"human\").write(user_input)\n",
        "\n",
        "    # Manually create a context string from history\n",
        "    context = \"Previous conversation:\\n\"\n",
        "    for msg in st.session_state.messages:\n",
        "        context += f\"{msg['role']}: {msg['content']}\\n\"\n",
        "\n",
        "    with st.chat_message(\"ai\"):\n",
        "        # Send context + new question\n",
        "        full_prompt = context + \"\\nPlease respond to the last question.\"\n",
        "        response = gemini.invoke(full_prompt)\n",
        "        st.write(response.content)\n",
        "\n",
        "        # Add AI response to history\n",
        "        st.session_state.messages.append({\"role\": \"ai\", \"content\": response.content})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgYHo7QwNULa",
        "outputId": "cb3fed68-7a73-438f-b51a-b978c5d562f1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing manual_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run manual_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "9bqZgBFiNg_d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dbfa636-c664-46a2-e95a-6862d3fad60b",
        "id": "ZCA-ShfgNg_e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://2582-34-138-160-11.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "lX0v5KEeNg_f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b98601-ea93-4fd6-ab25-7db7fdbae7e7",
        "id": "TnlrPbd6Ng_g"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        4795       1  0 09:28 ?        00:00:03 /usr/bin/python3 /usr/local/bin/streamlit run manual_app.py --server.port=8989\n",
            "root        6852    1533  0 09:37 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        6854    6852  0 09:37 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 4795"
      ],
      "metadata": {
        "id": "e46FJ2B9Ng_g"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Full LangChain Integration"
      ],
      "metadata": {
        "id": "RCMshwB1U9iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile langchain_app.py\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from operator import itemgetter\n",
        "import streamlit as st\n",
        "\n",
        "# Customize initial app landing page\n",
        "st.set_page_config(page_title=\"AI Assistant\", page_icon=\"🤖\")\n",
        "st.title(\"Welcome I am AI Assistant 🤖\")\n",
        "\n",
        "# Load a connection to Gemini LLM\n",
        "gemini = ChatGoogleGenerativeAI(model='gemini-2.0-flash-001',\n",
        "                               temperature=0.1,\n",
        "                               convert_system_message_to_human=True)\n",
        "\n",
        "# Add a basic system prompt for LLM behavior\n",
        "SYS_PROMPT = \"\"\"\n",
        "              Act as a helpful assistant and answer questions to the best of your ability.\n",
        "              Do not make up answers.\n",
        "              \"\"\"\n",
        "\n",
        "# Create a prompt template for langchain to use history to answer user prompts\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "  [\n",
        "    (\"system\", SYS_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "  ]\n",
        ")\n",
        "\n",
        "# Create a basic llm chain\n",
        "llm_chain = (\n",
        "  prompt\n",
        "  |\n",
        "  gemini\n",
        ")\n",
        "\n",
        "# Store conversation history in Streamlit session state\n",
        "streamlit_msg_history = StreamlitChatMessageHistory()\n",
        "\n",
        "# Create a conversation chain\n",
        "conversation_chain = RunnableWithMessageHistory(\n",
        "  llm_chain,\n",
        "  lambda session_id: streamlit_msg_history,  # Accesses memory\n",
        "  input_messages_key=\"input\",\n",
        "  history_messages_key=\"history\",\n",
        ")\n",
        "\n",
        "# Render current messages from StreamlitChatMessageHistory\n",
        "for msg in streamlit_msg_history.messages:\n",
        "  st.chat_message(msg.type).write(msg.content)\n",
        "\n",
        "# If user inputs a new prompt, display it and show the response\n",
        "if user_prompt := st.chat_input():\n",
        "  st.chat_message(\"human\").write(user_prompt)\n",
        "  # This is where response from the LLM is shown\n",
        "  with st.chat_message(\"ai\"):\n",
        "    config = {\"configurable\": {\"session_id\": \"any\"}}\n",
        "    # Get llm response\n",
        "    response = conversation_chain.invoke({\"input\": user_prompt}, config)\n",
        "    st.markdown(response.content) # Display response directly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLpP0t_9ojVI",
        "outputId": "f806671e-0f2a-4adc-f82a-c5b8f4190ef6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing langchain_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start the app"
      ],
      "metadata": {
        "id": "8de1tM6FVLsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run langchain_app.py --server.port=8989 &>./logs.txt &"
      ],
      "metadata": {
        "id": "Za_TAI2RkPI9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Terminate open tunnels if exist\n",
        "ngrok.kill()\n",
        "\n",
        "ngrok.set_auth_token(userdata.get('NGROK_API_KEY'))\n",
        "\n",
        "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
        "ngrok_tunnel = ngrok.connect(8989)\n",
        "print(\"Streamlit App:\", ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "id": "FrVhyQVirAqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f26885b-613e-48bb-b9a1-842870fb8818"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit App: https://bc26-34-138-160-11.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove running app processes"
      ],
      "metadata": {
        "id": "2Q3yFB_jsgC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "pG7Abg_LrAw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -ef | grep streamlit"
      ],
      "metadata": {
        "id": "x2VA4ZtCkPNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54da793a-c0a7-4ae9-cb63-eacfe66838da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       54456       1  7 14:25 ?        00:00:03 /usr/bin/python3 /usr/local/bin/streamlit run langchain_app.py --server.port=8989\n",
            "root       54654   13964  0 14:26 ?        00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root       54656   54654  0 14:26 ?        00:00:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo kill -9 54456"
      ],
      "metadata": {
        "id": "_WxGAxGHkPLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MlBQh5fdkPPY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}